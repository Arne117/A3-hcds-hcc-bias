{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course Human-Centered Data Science ([HCDS](https://www.mi.fu-berlin.de/en/inf/groups/hcc/teaching/winter_term_2020_21/course_human_centered_data_science.html)) - Winter Term 2020/21 - [HCC](https://www.mi.fu-berlin.de/en/inf/groups/hcc/index.html) | [Freie Universität Berlin](https://www.fu-berlin.de/)\n",
    "\n",
    "***\n",
    "\n",
    "# A2 - Wikipedia, ORES, and Bias in Data\n",
    "Please follow the reproducability workflow as practiced during the last exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1⃣ | Data acquisition\n",
    "\n",
    "You will use two data sources: (1) Wikipedia articles of politicians and (2) world population data.\n",
    "\n",
    "**Wikipedia articles -**\n",
    "The Wikipedia articles can be found on [Figshare](https://figshare.com/articles/Untitled_Item/5513449). It contains politiciaans by country from the English-language wikipedia. Please read through the documentation for this repository, then download and unzip it to extract the data file, which is called `page_data.csv`.\n",
    "\n",
    "**Population data -**\n",
    "The population data is available in `CSV` format in the `_data` folder. The file is named `export_2019.csv`. This dataset is drawn from the [world population datasheet](https://www.prb.org/international/indicator/population/table/) published by the Population Reference Bureau (downloaded 2020-11-13 10:14 AM). I have edited the dataset to make it easier to use in this assignment. The population per country is given in millions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step we need to import the neccessary libaries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the csv files as pandas dataframes for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population_df = pd.read_csv('../data_raw/export_2019.csv', delimiter=';')\n",
    "articles_df = pd.read_csv('../data_raw/page_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all files from a folder\n",
    "# files = [file for file in os.listdir('../data_raw')]\n",
    "# combined = pd.DataFrame()\n",
    "\n",
    "# for file in files:\n",
    "#     delimiter = ',' if file != 'export_2019.csv' else ';'\n",
    "#     current_df = pd.read_csv('../data_raw/' + file, delimiter=delimiter)\n",
    "#     combined = pd.concat([combined, current_df])\n",
    "# combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2⃣ | Data processing and cleaning\n",
    "The data in `page_data.csv` contain some rows that you will need to filter out. It contains some page names that start with the string `\"Template:\"`. These pages are not Wikipedia articles, and should not be included in your analysis. The data in `export_2019.csv` does not need any cleaning.\n",
    "\n",
    "***\n",
    "\n",
    "| | `page_data.csv` | | |\n",
    "|-|------|---------|--------|\n",
    "| | **page** | **country** | **rev_id** |\n",
    "|0|\tTemplate:ZambiaProvincialMinisters | Zambia | 235107991 |\n",
    "|1|\tBir I of Kanem | Chad | 355319463 |\n",
    "\n",
    "***\n",
    "\n",
    "| | `export_2019.csv` | | |\n",
    "|-|------|---------|--------|\n",
    "| | **country** | **population** | **region** |\n",
    "|0|\tAlgeria | 44.357 | AFRICA |\n",
    "|1|\tEgypt | 100.803 | 355319463 |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the Wikipedia articles of every row containing the `Template:` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Information Minister of the Palestinian Nation...</td>\n",
       "      <td>Palestinian Territory</td>\n",
       "      <td>393276188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yos Por</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>393822005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Julius Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395521877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Edvard Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395526568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47192</th>\n",
       "      <td>Yahya Jammeh</td>\n",
       "      <td>Gambia</td>\n",
       "      <td>807482007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47193</th>\n",
       "      <td>Lucius Fairchild</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47194</th>\n",
       "      <td>Fahd of Saudi Arabia</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>807483153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47195</th>\n",
       "      <td>Francis Fessenden</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47196</th>\n",
       "      <td>Ajay Kannoujiya</td>\n",
       "      <td>India</td>\n",
       "      <td>807484325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46701 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    page  \\\n",
       "1                                         Bir I of Kanem   \n",
       "10     Information Minister of the Palestinian Nation...   \n",
       "12                                               Yos Por   \n",
       "23                                          Julius Gregr   \n",
       "24                                          Edvard Gregr   \n",
       "...                                                  ...   \n",
       "47192                                       Yahya Jammeh   \n",
       "47193                                   Lucius Fairchild   \n",
       "47194                               Fahd of Saudi Arabia   \n",
       "47195                                  Francis Fessenden   \n",
       "47196                                    Ajay Kannoujiya   \n",
       "\n",
       "                     country     rev_id  \n",
       "1                       Chad  355319463  \n",
       "10     Palestinian Territory  393276188  \n",
       "12                  Cambodia  393822005  \n",
       "23            Czech Republic  395521877  \n",
       "24            Czech Republic  395526568  \n",
       "...                      ...        ...  \n",
       "47192                 Gambia  807482007  \n",
       "47193          United States  807483006  \n",
       "47194           Saudi Arabia  807483153  \n",
       "47195          United States  807483270  \n",
       "47196                  India  807484325  \n",
       "\n",
       "[46701 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = articles_df[~articles_df['page'].str.contains('Template:')]\n",
    "articles_df\n",
    "\n",
    "# sort out nan's\n",
    "# new_df = data_df[data_df.isna().any(axis=1)]\n",
    "# df.dropna(how='all')\n",
    "\n",
    "# convert to int\n",
    "# df['col'] = pd.to_numeric(df['col'])\n",
    "\n",
    "# run function on df\n",
    "# def get_city(address):\n",
    "#   return address.split(',')[1]\n",
    "# df['col'].apply(lambda addr: get_city(addr))\n",
    "\n",
    "# python f strings\n",
    "# .apply(lambda addr: f\"{get_city(addr)} xy\")\n",
    "\n",
    "# plot labeling\n",
    "# plt.xticks(df['x'].unique(), rotation='vertical', size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting article quality predictions with ORES\n",
    "\n",
    "Now you need to get the predicted quality scores for each article in the Wikipedia dataset. We're using a machine learning system called [**ORES**](https://www.mediawiki.org/wiki/ORES) (\"Objective Revision Evaluation Service\"). ORES estimates the quality of an article (at a particular point in time), and assigns a series of probabilities that the article is in one of the six quality categories. The options are, from best to worst:\n",
    "\n",
    "| ID | Quality Category |  Explanation |\n",
    "|----|------------------|----------|\n",
    "| 1 | FA    | Featured article |\n",
    "| 2 | GA    | Good article |\n",
    "| 3 | B     | B-class article |\n",
    "| 4 | C     | C-class article |\n",
    "| 5 | Start | Start-class article |\n",
    "| 6 | Stub  | Stub-class article |\n",
    "\n",
    "For context, these quality classes are a sub-set of quality assessment categories developed by Wikipedia editors. If you're curious, you can [read more](https://en.wikipedia.org/wiki/Wikipedia:Content_assessment#Grades) about what these assessment classes mean on English Wikipedia. For this assignment, you only need to know that these categories exist, and that ORES will assign one of these six categories to any `rev_id`. You need to extract all `rev_id`s in the `page_data.csv` file and use the ORES API to get the predicted quality score for that specific article revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORES REST API endpoint\n",
    "\n",
    "The [ORES REST API](https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context_revid_model) is configured fairly similarly to the pageviews API we used for the last assignment. It expects the following parameters:\n",
    "\n",
    "* **project** --> `enwiki`\n",
    "* **revid** --> e.g. `235107991` or multiple ids e.g.: `235107991|355319463` (batch)\n",
    "* **model** --> `wp10` - The name of a model to use when scoring.\n",
    "\n",
    "**❗Note on batch processing:** Please read the documentation about [API usage](https://www.mediawiki.org/wiki/ORES#API_usage) if you want to query a large number of revisions (batches). \n",
    "\n",
    "You will notice that ORES returns a prediction value that contains the name of one category (e.g. `Start`), as well as probability values for each of the six quality categories. For this assignment, you only need to capture and use the value for prediction.\n",
    "\n",
    "**❗Note:** It's possible that you will be unable to get a score for a particular article. If that happens, make sure to maintain a log of articles for which you were not able to retrieve an ORES score. This log should be saved as a separate file named `ORES_no_scores.csv` and should include the `page`, `country`, and `rev_id` (just as in `page_data.csv`).\n",
    "\n",
    "You can use the following **samle code for API calls**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting chunk 1 of 943\n",
      "Requesting chunk 2 of 943\n",
      "Requesting chunk 3 of 943\n",
      "Requesting chunk 4 of 943\n",
      "Requesting chunk 5 of 943\n",
      "Requesting chunk 6 of 943\n",
      "Requesting chunk 7 of 943\n",
      "Requesting chunk 8 of 943\n",
      "Requesting chunk 9 of 943\n",
      "Requesting chunk 10 of 943\n",
      "Requesting chunk 11 of 943\n",
      "Requesting chunk 12 of 943\n",
      "Requesting chunk 13 of 943\n",
      "Requesting chunk 14 of 943\n",
      "Requesting chunk 15 of 943\n",
      "Requesting chunk 16 of 943\n",
      "Requesting chunk 17 of 943\n",
      "Requesting chunk 18 of 943\n",
      "Requesting chunk 19 of 943\n",
      "Requesting chunk 20 of 943\n",
      "Requesting chunk 21 of 943\n",
      "Requesting chunk 22 of 943\n",
      "Requesting chunk 23 of 943\n",
      "Requesting chunk 24 of 943\n",
      "Requesting chunk 25 of 943\n",
      "Requesting chunk 26 of 943\n",
      "Requesting chunk 27 of 943\n",
      "Requesting chunk 28 of 943\n",
      "Requesting chunk 29 of 943\n",
      "Requesting chunk 30 of 943\n",
      "Requesting chunk 31 of 943\n",
      "Requesting chunk 32 of 943\n",
      "Requesting chunk 33 of 943\n",
      "Requesting chunk 34 of 943\n",
      "Requesting chunk 35 of 943\n",
      "Requesting chunk 36 of 943\n",
      "Requesting chunk 37 of 943\n",
      "Requesting chunk 38 of 943\n",
      "Requesting chunk 39 of 943\n",
      "Requesting chunk 40 of 943\n",
      "Requesting chunk 41 of 943\n",
      "Requesting chunk 42 of 943\n",
      "Requesting chunk 43 of 943\n",
      "Requesting chunk 44 of 943\n",
      "Requesting chunk 45 of 943\n",
      "Requesting chunk 46 of 943\n",
      "Requesting chunk 47 of 943\n",
      "Requesting chunk 48 of 943\n",
      "Requesting chunk 49 of 943\n",
      "Requesting chunk 50 of 943\n",
      "Requesting chunk 51 of 943\n",
      "Requesting chunk 52 of 943\n",
      "Requesting chunk 53 of 943\n",
      "Requesting chunk 54 of 943\n",
      "Requesting chunk 55 of 943\n",
      "Requesting chunk 56 of 943\n",
      "Requesting chunk 57 of 943\n",
      "Requesting chunk 58 of 943\n",
      "Requesting chunk 59 of 943\n",
      "Requesting chunk 60 of 943\n",
      "Requesting chunk 61 of 943\n",
      "Requesting chunk 62 of 943\n",
      "Requesting chunk 63 of 943\n",
      "Requesting chunk 64 of 943\n",
      "Requesting chunk 65 of 943\n",
      "Requesting chunk 66 of 943\n",
      "Requesting chunk 67 of 943\n",
      "Requesting chunk 68 of 943\n",
      "Requesting chunk 69 of 943\n",
      "Requesting chunk 70 of 943\n",
      "Requesting chunk 71 of 943\n",
      "Requesting chunk 72 of 943\n",
      "Requesting chunk 73 of 943\n",
      "Requesting chunk 74 of 943\n",
      "Requesting chunk 75 of 943\n",
      "Requesting chunk 76 of 943\n",
      "Requesting chunk 77 of 943\n",
      "Requesting chunk 78 of 943\n",
      "Requesting chunk 79 of 943\n",
      "Requesting chunk 80 of 943\n",
      "Requesting chunk 81 of 943\n",
      "Requesting chunk 82 of 943\n",
      "Requesting chunk 83 of 943\n",
      "Requesting chunk 84 of 943\n",
      "Requesting chunk 85 of 943\n",
      "Requesting chunk 86 of 943\n",
      "Requesting chunk 87 of 943\n",
      "Requesting chunk 88 of 943\n",
      "Requesting chunk 89 of 943\n",
      "Requesting chunk 90 of 943\n",
      "Requesting chunk 91 of 943\n",
      "Requesting chunk 92 of 943\n",
      "Requesting chunk 93 of 943\n",
      "Requesting chunk 94 of 943\n",
      "Requesting chunk 95 of 943\n",
      "Requesting chunk 96 of 943\n",
      "Requesting chunk 97 of 943\n",
      "Requesting chunk 98 of 943\n",
      "Requesting chunk 99 of 943\n",
      "Requesting chunk 100 of 943\n",
      "Requesting chunk 101 of 943\n",
      "Requesting chunk 102 of 943\n",
      "Requesting chunk 103 of 943\n",
      "Requesting chunk 104 of 943\n",
      "Requesting chunk 105 of 943\n",
      "Requesting chunk 106 of 943\n",
      "Requesting chunk 107 of 943\n",
      "Requesting chunk 108 of 943\n",
      "Requesting chunk 109 of 943\n",
      "Requesting chunk 110 of 943\n",
      "Requesting chunk 111 of 943\n",
      "Requesting chunk 112 of 943\n",
      "Requesting chunk 113 of 943\n",
      "Requesting chunk 114 of 943\n",
      "Requesting chunk 115 of 943\n",
      "Requesting chunk 116 of 943\n",
      "Requesting chunk 117 of 943\n",
      "Requesting chunk 118 of 943\n",
      "Requesting chunk 119 of 943\n",
      "Requesting chunk 120 of 943\n",
      "Requesting chunk 121 of 943\n",
      "Requesting chunk 122 of 943\n",
      "Requesting chunk 123 of 943\n",
      "Requesting chunk 124 of 943\n",
      "Requesting chunk 125 of 943\n",
      "Requesting chunk 126 of 943\n",
      "Requesting chunk 127 of 943\n",
      "Requesting chunk 128 of 943\n",
      "Requesting chunk 129 of 943\n",
      "Requesting chunk 130 of 943\n",
      "Requesting chunk 131 of 943\n",
      "Requesting chunk 132 of 943\n",
      "Requesting chunk 133 of 943\n",
      "Requesting chunk 134 of 943\n",
      "Requesting chunk 135 of 943\n",
      "Requesting chunk 136 of 943\n",
      "Requesting chunk 137 of 943\n",
      "Requesting chunk 138 of 943\n",
      "Requesting chunk 139 of 943\n",
      "Requesting chunk 140 of 943\n",
      "Requesting chunk 141 of 943\n",
      "Requesting chunk 142 of 943\n",
      "Requesting chunk 143 of 943\n",
      "Requesting chunk 144 of 943\n",
      "Requesting chunk 145 of 943\n",
      "Requesting chunk 146 of 943\n",
      "Requesting chunk 147 of 943\n",
      "Requesting chunk 148 of 943\n",
      "Requesting chunk 149 of 943\n",
      "Requesting chunk 150 of 943\n",
      "Requesting chunk 151 of 943\n",
      "Requesting chunk 152 of 943\n",
      "Requesting chunk 153 of 943\n",
      "Requesting chunk 154 of 943\n",
      "Requesting chunk 155 of 943\n",
      "Requesting chunk 156 of 943\n",
      "Requesting chunk 157 of 943\n",
      "Requesting chunk 158 of 943\n",
      "Requesting chunk 159 of 943\n",
      "Requesting chunk 160 of 943\n",
      "Requesting chunk 161 of 943\n",
      "Requesting chunk 162 of 943\n",
      "Requesting chunk 163 of 943\n",
      "Requesting chunk 164 of 943\n",
      "Requesting chunk 165 of 943\n",
      "Requesting chunk 166 of 943\n",
      "Requesting chunk 167 of 943\n",
      "Requesting chunk 168 of 943\n",
      "Requesting chunk 169 of 943\n",
      "Requesting chunk 170 of 943\n",
      "Requesting chunk 171 of 943\n",
      "Requesting chunk 172 of 943\n",
      "Requesting chunk 173 of 943\n",
      "Requesting chunk 174 of 943\n",
      "Requesting chunk 175 of 943\n",
      "Requesting chunk 176 of 943\n",
      "Requesting chunk 177 of 943\n",
      "Requesting chunk 178 of 943\n",
      "Requesting chunk 179 of 943\n",
      "Requesting chunk 180 of 943\n",
      "Requesting chunk 181 of 943\n",
      "Requesting chunk 182 of 943\n",
      "Requesting chunk 183 of 943\n",
      "Requesting chunk 184 of 943\n",
      "Requesting chunk 185 of 943\n",
      "Requesting chunk 186 of 943\n",
      "Requesting chunk 187 of 943\n",
      "Requesting chunk 188 of 943\n",
      "Requesting chunk 189 of 943\n",
      "Requesting chunk 190 of 943\n",
      "Requesting chunk 191 of 943\n",
      "Requesting chunk 192 of 943\n",
      "Requesting chunk 193 of 943\n",
      "Requesting chunk 194 of 943\n",
      "Requesting chunk 195 of 943\n",
      "Requesting chunk 196 of 943\n",
      "Requesting chunk 197 of 943\n",
      "Requesting chunk 198 of 943\n",
      "Requesting chunk 199 of 943\n",
      "Requesting chunk 200 of 943\n",
      "Requesting chunk 201 of 943\n",
      "Requesting chunk 202 of 943\n",
      "Requesting chunk 203 of 943\n",
      "Requesting chunk 204 of 943\n",
      "Requesting chunk 205 of 943\n",
      "Requesting chunk 206 of 943\n",
      "Requesting chunk 207 of 943\n",
      "Requesting chunk 208 of 943\n",
      "Requesting chunk 209 of 943\n",
      "Requesting chunk 210 of 943\n",
      "Requesting chunk 211 of 943\n",
      "Requesting chunk 212 of 943\n",
      "Requesting chunk 213 of 943\n",
      "Requesting chunk 214 of 943\n",
      "Requesting chunk 215 of 943\n",
      "Requesting chunk 216 of 943\n",
      "Requesting chunk 217 of 943\n",
      "Requesting chunk 218 of 943\n",
      "Requesting chunk 219 of 943\n",
      "Requesting chunk 220 of 943\n",
      "Requesting chunk 221 of 943\n",
      "Requesting chunk 222 of 943\n",
      "Requesting chunk 223 of 943\n",
      "Requesting chunk 224 of 943\n",
      "Requesting chunk 225 of 943\n",
      "Requesting chunk 226 of 943\n",
      "Requesting chunk 227 of 943\n",
      "Requesting chunk 228 of 943\n",
      "Requesting chunk 229 of 943\n",
      "Requesting chunk 230 of 943\n",
      "Requesting chunk 231 of 943\n",
      "Requesting chunk 232 of 943\n",
      "Requesting chunk 233 of 943\n",
      "Requesting chunk 234 of 943\n",
      "Requesting chunk 235 of 943\n",
      "Requesting chunk 236 of 943\n",
      "Requesting chunk 237 of 943\n",
      "Requesting chunk 238 of 943\n",
      "Requesting chunk 239 of 943\n",
      "Requesting chunk 240 of 943\n",
      "Requesting chunk 241 of 943\n",
      "Requesting chunk 242 of 943\n",
      "Requesting chunk 243 of 943\n",
      "Requesting chunk 244 of 943\n",
      "Requesting chunk 245 of 943\n",
      "Requesting chunk 246 of 943\n",
      "Requesting chunk 247 of 943\n",
      "Requesting chunk 248 of 943\n",
      "Requesting chunk 249 of 943\n",
      "Requesting chunk 250 of 943\n",
      "Requesting chunk 251 of 943\n",
      "Requesting chunk 252 of 943\n",
      "Requesting chunk 253 of 943\n",
      "Requesting chunk 254 of 943\n",
      "Requesting chunk 255 of 943\n",
      "Requesting chunk 256 of 943\n",
      "Requesting chunk 257 of 943\n",
      "Requesting chunk 258 of 943\n",
      "Requesting chunk 259 of 943\n",
      "Requesting chunk 260 of 943\n",
      "Requesting chunk 261 of 943\n",
      "Requesting chunk 262 of 943\n",
      "Requesting chunk 263 of 943\n",
      "Requesting chunk 264 of 943\n",
      "Requesting chunk 265 of 943\n",
      "Requesting chunk 266 of 943\n",
      "Requesting chunk 267 of 943\n",
      "Requesting chunk 268 of 943\n",
      "Requesting chunk 269 of 943\n",
      "Requesting chunk 270 of 943\n",
      "Requesting chunk 271 of 943\n",
      "Requesting chunk 272 of 943\n",
      "Requesting chunk 273 of 943\n",
      "Requesting chunk 274 of 943\n",
      "Requesting chunk 275 of 943\n",
      "Requesting chunk 276 of 943\n",
      "Requesting chunk 277 of 943\n",
      "Requesting chunk 278 of 943\n",
      "Requesting chunk 279 of 943\n",
      "Requesting chunk 280 of 943\n",
      "Requesting chunk 281 of 943\n",
      "Requesting chunk 282 of 943\n",
      "Requesting chunk 283 of 943\n",
      "Requesting chunk 284 of 943\n",
      "Requesting chunk 285 of 943\n",
      "Requesting chunk 286 of 943\n",
      "Requesting chunk 287 of 943\n",
      "Requesting chunk 288 of 943\n",
      "Requesting chunk 289 of 943\n",
      "Requesting chunk 290 of 943\n",
      "Requesting chunk 291 of 943\n",
      "Requesting chunk 292 of 943\n",
      "Requesting chunk 293 of 943\n",
      "Requesting chunk 294 of 943\n",
      "Requesting chunk 295 of 943\n",
      "Requesting chunk 296 of 943\n",
      "Requesting chunk 297 of 943\n",
      "Requesting chunk 298 of 943\n",
      "Requesting chunk 299 of 943\n",
      "Requesting chunk 300 of 943\n",
      "Requesting chunk 301 of 943\n",
      "Requesting chunk 302 of 943\n",
      "Requesting chunk 303 of 943\n",
      "Requesting chunk 304 of 943\n",
      "Requesting chunk 305 of 943\n",
      "Requesting chunk 306 of 943\n",
      "Requesting chunk 307 of 943\n",
      "Requesting chunk 308 of 943\n",
      "Requesting chunk 309 of 943\n",
      "Requesting chunk 310 of 943\n",
      "Requesting chunk 311 of 943\n",
      "Requesting chunk 312 of 943\n",
      "Requesting chunk 313 of 943\n",
      "Requesting chunk 314 of 943\n",
      "Requesting chunk 315 of 943\n",
      "Requesting chunk 316 of 943\n",
      "Requesting chunk 317 of 943\n",
      "Requesting chunk 318 of 943\n",
      "Requesting chunk 319 of 943\n",
      "Requesting chunk 320 of 943\n",
      "Requesting chunk 321 of 943\n",
      "Requesting chunk 322 of 943\n",
      "Requesting chunk 323 of 943\n",
      "Requesting chunk 324 of 943\n",
      "Requesting chunk 325 of 943\n",
      "Requesting chunk 326 of 943\n",
      "Requesting chunk 327 of 943\n",
      "Requesting chunk 328 of 943\n",
      "Requesting chunk 329 of 943\n",
      "Requesting chunk 330 of 943\n",
      "Requesting chunk 331 of 943\n",
      "Requesting chunk 332 of 943\n",
      "Requesting chunk 333 of 943\n",
      "Requesting chunk 334 of 943\n",
      "Requesting chunk 335 of 943\n",
      "Requesting chunk 336 of 943\n",
      "Requesting chunk 337 of 943\n",
      "Requesting chunk 338 of 943\n",
      "Requesting chunk 339 of 943\n",
      "Requesting chunk 340 of 943\n",
      "Requesting chunk 341 of 943\n",
      "Requesting chunk 342 of 943\n",
      "Requesting chunk 343 of 943\n",
      "Requesting chunk 344 of 943\n",
      "Requesting chunk 345 of 943\n",
      "Requesting chunk 346 of 943\n",
      "Requesting chunk 347 of 943\n",
      "Requesting chunk 348 of 943\n",
      "Requesting chunk 349 of 943\n",
      "Requesting chunk 350 of 943\n",
      "Requesting chunk 351 of 943\n",
      "Requesting chunk 352 of 943\n",
      "Requesting chunk 353 of 943\n",
      "Requesting chunk 354 of 943\n",
      "Requesting chunk 355 of 943\n",
      "Requesting chunk 356 of 943\n",
      "Requesting chunk 357 of 943\n",
      "Requesting chunk 358 of 943\n",
      "Requesting chunk 359 of 943\n",
      "Requesting chunk 360 of 943\n",
      "Requesting chunk 361 of 943\n",
      "Requesting chunk 362 of 943\n",
      "Requesting chunk 363 of 943\n",
      "Requesting chunk 364 of 943\n",
      "Requesting chunk 365 of 943\n",
      "Requesting chunk 366 of 943\n",
      "Requesting chunk 367 of 943\n",
      "Requesting chunk 368 of 943\n",
      "Requesting chunk 369 of 943\n",
      "Requesting chunk 370 of 943\n",
      "Requesting chunk 371 of 943\n",
      "Requesting chunk 372 of 943\n",
      "Requesting chunk 373 of 943\n",
      "Requesting chunk 374 of 943\n",
      "Requesting chunk 375 of 943\n",
      "Requesting chunk 376 of 943\n",
      "Requesting chunk 377 of 943\n",
      "Requesting chunk 378 of 943\n",
      "Requesting chunk 379 of 943\n",
      "Requesting chunk 380 of 943\n",
      "Requesting chunk 381 of 943\n",
      "Requesting chunk 382 of 943\n",
      "Requesting chunk 383 of 943\n",
      "Requesting chunk 384 of 943\n",
      "Requesting chunk 385 of 943\n",
      "Requesting chunk 386 of 943\n",
      "Requesting chunk 387 of 943\n",
      "Requesting chunk 388 of 943\n",
      "Requesting chunk 389 of 943\n",
      "Requesting chunk 390 of 943\n",
      "Requesting chunk 391 of 943\n",
      "Requesting chunk 392 of 943\n",
      "Requesting chunk 393 of 943\n",
      "Requesting chunk 394 of 943\n",
      "Requesting chunk 395 of 943\n",
      "Requesting chunk 396 of 943\n",
      "Requesting chunk 397 of 943\n",
      "Requesting chunk 398 of 943\n",
      "Requesting chunk 399 of 943\n",
      "Requesting chunk 400 of 943\n",
      "Requesting chunk 401 of 943\n",
      "Requesting chunk 402 of 943\n",
      "Requesting chunk 403 of 943\n",
      "Requesting chunk 404 of 943\n",
      "Requesting chunk 405 of 943\n",
      "Requesting chunk 406 of 943\n",
      "Requesting chunk 407 of 943\n",
      "Requesting chunk 408 of 943\n",
      "Requesting chunk 409 of 943\n",
      "Requesting chunk 410 of 943\n",
      "Requesting chunk 411 of 943\n",
      "Requesting chunk 412 of 943\n",
      "Requesting chunk 413 of 943\n",
      "Requesting chunk 414 of 943\n",
      "Requesting chunk 415 of 943\n",
      "Requesting chunk 416 of 943\n",
      "Requesting chunk 417 of 943\n",
      "Requesting chunk 418 of 943\n",
      "Requesting chunk 419 of 943\n",
      "Requesting chunk 420 of 943\n",
      "Requesting chunk 421 of 943\n",
      "Requesting chunk 422 of 943\n",
      "Requesting chunk 423 of 943\n",
      "Requesting chunk 424 of 943\n",
      "Requesting chunk 425 of 943\n",
      "Requesting chunk 426 of 943\n",
      "Requesting chunk 427 of 943\n",
      "Requesting chunk 428 of 943\n",
      "Requesting chunk 429 of 943\n",
      "Requesting chunk 430 of 943\n",
      "Requesting chunk 431 of 943\n",
      "Requesting chunk 432 of 943\n",
      "Requesting chunk 433 of 943\n",
      "Requesting chunk 434 of 943\n",
      "Requesting chunk 435 of 943\n",
      "Requesting chunk 436 of 943\n",
      "Requesting chunk 437 of 943\n",
      "Requesting chunk 438 of 943\n",
      "Requesting chunk 439 of 943\n",
      "Requesting chunk 440 of 943\n",
      "Requesting chunk 441 of 943\n",
      "Requesting chunk 442 of 943\n",
      "Requesting chunk 443 of 943\n",
      "Requesting chunk 444 of 943\n",
      "Requesting chunk 445 of 943\n",
      "Requesting chunk 446 of 943\n",
      "Requesting chunk 447 of 943\n",
      "Requesting chunk 448 of 943\n",
      "Requesting chunk 449 of 943\n",
      "Requesting chunk 450 of 943\n",
      "Requesting chunk 451 of 943\n",
      "Requesting chunk 452 of 943\n",
      "Requesting chunk 453 of 943\n",
      "Requesting chunk 454 of 943\n",
      "Requesting chunk 455 of 943\n",
      "Requesting chunk 456 of 943\n",
      "Requesting chunk 457 of 943\n",
      "Requesting chunk 458 of 943\n",
      "Requesting chunk 459 of 943\n",
      "Requesting chunk 460 of 943\n",
      "Requesting chunk 461 of 943\n",
      "Requesting chunk 462 of 943\n",
      "Requesting chunk 463 of 943\n",
      "Requesting chunk 464 of 943\n",
      "Requesting chunk 465 of 943\n",
      "Requesting chunk 466 of 943\n",
      "Requesting chunk 467 of 943\n",
      "Requesting chunk 468 of 943\n",
      "Requesting chunk 469 of 943\n",
      "Requesting chunk 470 of 943\n",
      "Requesting chunk 471 of 943\n",
      "Requesting chunk 472 of 943\n",
      "Requesting chunk 473 of 943\n",
      "Requesting chunk 474 of 943\n",
      "Requesting chunk 475 of 943\n",
      "Requesting chunk 476 of 943\n",
      "Requesting chunk 477 of 943\n",
      "Requesting chunk 478 of 943\n",
      "Requesting chunk 479 of 943\n",
      "Requesting chunk 480 of 943\n",
      "Requesting chunk 481 of 943\n",
      "Requesting chunk 482 of 943\n",
      "Requesting chunk 483 of 943\n",
      "Requesting chunk 484 of 943\n",
      "Requesting chunk 485 of 943\n",
      "Requesting chunk 486 of 943\n",
      "Requesting chunk 487 of 943\n",
      "Requesting chunk 488 of 943\n",
      "Requesting chunk 489 of 943\n",
      "Requesting chunk 490 of 943\n",
      "Requesting chunk 491 of 943\n",
      "Requesting chunk 492 of 943\n",
      "Requesting chunk 493 of 943\n",
      "Requesting chunk 494 of 943\n",
      "Requesting chunk 495 of 943\n",
      "Requesting chunk 496 of 943\n",
      "Requesting chunk 497 of 943\n",
      "Requesting chunk 498 of 943\n",
      "Requesting chunk 499 of 943\n",
      "Requesting chunk 500 of 943\n",
      "Requesting chunk 501 of 943\n",
      "Requesting chunk 502 of 943\n",
      "Requesting chunk 503 of 943\n",
      "Requesting chunk 504 of 943\n",
      "Requesting chunk 505 of 943\n",
      "Requesting chunk 506 of 943\n",
      "Requesting chunk 507 of 943\n",
      "Requesting chunk 508 of 943\n",
      "Requesting chunk 509 of 943\n",
      "Requesting chunk 510 of 943\n",
      "Requesting chunk 511 of 943\n",
      "Requesting chunk 512 of 943\n",
      "Requesting chunk 513 of 943\n",
      "Requesting chunk 514 of 943\n",
      "Requesting chunk 515 of 943\n",
      "Requesting chunk 516 of 943\n",
      "Requesting chunk 517 of 943\n",
      "Requesting chunk 518 of 943\n",
      "Requesting chunk 519 of 943\n",
      "Requesting chunk 520 of 943\n",
      "Requesting chunk 521 of 943\n",
      "Requesting chunk 522 of 943\n",
      "Requesting chunk 523 of 943\n",
      "Requesting chunk 524 of 943\n",
      "Requesting chunk 525 of 943\n",
      "Requesting chunk 526 of 943\n",
      "Requesting chunk 527 of 943\n",
      "Requesting chunk 528 of 943\n",
      "Requesting chunk 529 of 943\n",
      "Requesting chunk 530 of 943\n",
      "Requesting chunk 531 of 943\n",
      "Requesting chunk 532 of 943\n",
      "Requesting chunk 533 of 943\n",
      "Requesting chunk 534 of 943\n",
      "Requesting chunk 535 of 943\n",
      "Requesting chunk 536 of 943\n",
      "Requesting chunk 537 of 943\n",
      "Requesting chunk 538 of 943\n",
      "Requesting chunk 539 of 943\n",
      "Requesting chunk 540 of 943\n",
      "Requesting chunk 541 of 943\n",
      "Requesting chunk 542 of 943\n",
      "Requesting chunk 543 of 943\n",
      "Requesting chunk 544 of 943\n",
      "Requesting chunk 545 of 943\n",
      "Requesting chunk 546 of 943\n",
      "Requesting chunk 547 of 943\n",
      "Requesting chunk 548 of 943\n",
      "Requesting chunk 549 of 943\n",
      "Requesting chunk 550 of 943\n",
      "Requesting chunk 551 of 943\n",
      "Requesting chunk 552 of 943\n",
      "Requesting chunk 553 of 943\n",
      "Requesting chunk 554 of 943\n",
      "Requesting chunk 555 of 943\n",
      "Requesting chunk 556 of 943\n",
      "Requesting chunk 557 of 943\n",
      "Requesting chunk 558 of 943\n",
      "Requesting chunk 559 of 943\n",
      "Requesting chunk 560 of 943\n",
      "Requesting chunk 561 of 943\n",
      "Requesting chunk 562 of 943\n",
      "Requesting chunk 563 of 943\n",
      "Requesting chunk 564 of 943\n",
      "Requesting chunk 565 of 943\n",
      "Requesting chunk 566 of 943\n",
      "Requesting chunk 567 of 943\n",
      "Requesting chunk 568 of 943\n",
      "Requesting chunk 569 of 943\n",
      "Requesting chunk 570 of 943\n",
      "Requesting chunk 571 of 943\n",
      "Requesting chunk 572 of 943\n",
      "Requesting chunk 573 of 943\n",
      "Requesting chunk 574 of 943\n",
      "Requesting chunk 575 of 943\n",
      "Requesting chunk 576 of 943\n",
      "Requesting chunk 577 of 943\n",
      "Requesting chunk 578 of 943\n",
      "Requesting chunk 579 of 943\n",
      "Requesting chunk 580 of 943\n",
      "Requesting chunk 581 of 943\n",
      "Requesting chunk 582 of 943\n",
      "Requesting chunk 583 of 943\n",
      "Requesting chunk 584 of 943\n",
      "Requesting chunk 585 of 943\n",
      "Requesting chunk 586 of 943\n",
      "Requesting chunk 587 of 943\n",
      "Requesting chunk 588 of 943\n",
      "Requesting chunk 589 of 943\n",
      "Requesting chunk 590 of 943\n",
      "Requesting chunk 591 of 943\n",
      "Requesting chunk 592 of 943\n",
      "Requesting chunk 593 of 943\n",
      "Requesting chunk 594 of 943\n",
      "Requesting chunk 595 of 943\n",
      "Requesting chunk 596 of 943\n",
      "Requesting chunk 597 of 943\n",
      "Requesting chunk 598 of 943\n",
      "Requesting chunk 599 of 943\n",
      "Requesting chunk 600 of 943\n",
      "Requesting chunk 601 of 943\n",
      "Requesting chunk 602 of 943\n",
      "Requesting chunk 603 of 943\n",
      "Requesting chunk 604 of 943\n",
      "Requesting chunk 605 of 943\n",
      "Requesting chunk 606 of 943\n",
      "Requesting chunk 607 of 943\n",
      "Requesting chunk 608 of 943\n",
      "Requesting chunk 609 of 943\n",
      "Requesting chunk 610 of 943\n",
      "Requesting chunk 611 of 943\n",
      "Requesting chunk 612 of 943\n",
      "Requesting chunk 613 of 943\n",
      "Requesting chunk 614 of 943\n",
      "Requesting chunk 615 of 943\n",
      "Requesting chunk 616 of 943\n",
      "Requesting chunk 617 of 943\n",
      "Requesting chunk 618 of 943\n",
      "Requesting chunk 619 of 943\n",
      "Requesting chunk 620 of 943\n",
      "Requesting chunk 621 of 943\n",
      "Requesting chunk 622 of 943\n",
      "Requesting chunk 623 of 943\n",
      "Requesting chunk 624 of 943\n",
      "Requesting chunk 625 of 943\n",
      "Requesting chunk 626 of 943\n",
      "Requesting chunk 627 of 943\n",
      "Requesting chunk 628 of 943\n",
      "Requesting chunk 629 of 943\n",
      "Requesting chunk 630 of 943\n",
      "Requesting chunk 631 of 943\n",
      "Requesting chunk 632 of 943\n",
      "Requesting chunk 633 of 943\n",
      "Requesting chunk 634 of 943\n",
      "Requesting chunk 635 of 943\n",
      "Requesting chunk 636 of 943\n",
      "Requesting chunk 637 of 943\n",
      "Requesting chunk 638 of 943\n",
      "Requesting chunk 639 of 943\n",
      "Requesting chunk 640 of 943\n",
      "Requesting chunk 641 of 943\n",
      "Requesting chunk 642 of 943\n",
      "Requesting chunk 643 of 943\n",
      "Requesting chunk 644 of 943\n",
      "Requesting chunk 645 of 943\n",
      "Requesting chunk 646 of 943\n",
      "Requesting chunk 647 of 943\n",
      "Requesting chunk 648 of 943\n",
      "Requesting chunk 649 of 943\n",
      "Requesting chunk 650 of 943\n",
      "Requesting chunk 651 of 943\n",
      "Requesting chunk 652 of 943\n",
      "Requesting chunk 653 of 943\n",
      "Requesting chunk 654 of 943\n",
      "Requesting chunk 655 of 943\n",
      "Requesting chunk 656 of 943\n",
      "Requesting chunk 657 of 943\n",
      "Requesting chunk 658 of 943\n",
      "Requesting chunk 659 of 943\n",
      "Requesting chunk 660 of 943\n",
      "Requesting chunk 661 of 943\n",
      "Requesting chunk 662 of 943\n",
      "Requesting chunk 663 of 943\n",
      "Requesting chunk 664 of 943\n",
      "Requesting chunk 665 of 943\n",
      "Requesting chunk 666 of 943\n",
      "Requesting chunk 667 of 943\n",
      "Requesting chunk 668 of 943\n",
      "Requesting chunk 669 of 943\n",
      "Requesting chunk 670 of 943\n",
      "Requesting chunk 671 of 943\n",
      "Requesting chunk 672 of 943\n",
      "Requesting chunk 673 of 943\n",
      "Requesting chunk 674 of 943\n",
      "Requesting chunk 675 of 943\n",
      "Requesting chunk 676 of 943\n",
      "Requesting chunk 677 of 943\n",
      "Requesting chunk 678 of 943\n",
      "Requesting chunk 679 of 943\n",
      "Requesting chunk 680 of 943\n",
      "Requesting chunk 681 of 943\n",
      "Requesting chunk 682 of 943\n",
      "Requesting chunk 683 of 943\n",
      "Requesting chunk 684 of 943\n",
      "Requesting chunk 685 of 943\n",
      "Requesting chunk 686 of 943\n",
      "Requesting chunk 687 of 943\n",
      "Requesting chunk 688 of 943\n",
      "Requesting chunk 689 of 943\n",
      "Requesting chunk 690 of 943\n",
      "Requesting chunk 691 of 943\n",
      "Requesting chunk 692 of 943\n",
      "Requesting chunk 693 of 943\n",
      "Requesting chunk 694 of 943\n",
      "Requesting chunk 695 of 943\n",
      "Requesting chunk 696 of 943\n",
      "Requesting chunk 697 of 943\n",
      "Requesting chunk 698 of 943\n",
      "Requesting chunk 699 of 943\n",
      "Requesting chunk 700 of 943\n",
      "Requesting chunk 701 of 943\n",
      "Requesting chunk 702 of 943\n",
      "Requesting chunk 703 of 943\n",
      "Requesting chunk 704 of 943\n",
      "Requesting chunk 705 of 943\n",
      "Requesting chunk 706 of 943\n",
      "Requesting chunk 707 of 943\n",
      "Requesting chunk 708 of 943\n",
      "Requesting chunk 709 of 943\n",
      "Requesting chunk 710 of 943\n",
      "Requesting chunk 711 of 943\n",
      "Requesting chunk 712 of 943\n",
      "Requesting chunk 713 of 943\n",
      "Requesting chunk 714 of 943\n",
      "Requesting chunk 715 of 943\n",
      "Requesting chunk 716 of 943\n",
      "Requesting chunk 717 of 943\n",
      "Requesting chunk 718 of 943\n",
      "Requesting chunk 719 of 943\n",
      "Requesting chunk 720 of 943\n",
      "Requesting chunk 721 of 943\n",
      "Requesting chunk 722 of 943\n",
      "Requesting chunk 723 of 943\n",
      "Requesting chunk 724 of 943\n",
      "Requesting chunk 725 of 943\n",
      "Requesting chunk 726 of 943\n",
      "Requesting chunk 727 of 943\n",
      "Requesting chunk 728 of 943\n",
      "Requesting chunk 729 of 943\n",
      "Requesting chunk 730 of 943\n",
      "Requesting chunk 731 of 943\n",
      "Requesting chunk 732 of 943\n",
      "Requesting chunk 733 of 943\n",
      "Requesting chunk 734 of 943\n",
      "Requesting chunk 735 of 943\n",
      "Requesting chunk 736 of 943\n",
      "Requesting chunk 737 of 943\n",
      "Requesting chunk 738 of 943\n",
      "Requesting chunk 739 of 943\n",
      "Requesting chunk 740 of 943\n",
      "Requesting chunk 741 of 943\n",
      "Requesting chunk 742 of 943\n",
      "Requesting chunk 743 of 943\n",
      "Requesting chunk 744 of 943\n",
      "Requesting chunk 745 of 943\n",
      "Requesting chunk 746 of 943\n",
      "Requesting chunk 747 of 943\n",
      "Requesting chunk 748 of 943\n",
      "Requesting chunk 749 of 943\n",
      "Requesting chunk 750 of 943\n",
      "Requesting chunk 751 of 943\n",
      "Requesting chunk 752 of 943\n",
      "Requesting chunk 753 of 943\n",
      "Requesting chunk 754 of 943\n",
      "Requesting chunk 755 of 943\n",
      "Requesting chunk 756 of 943\n",
      "Requesting chunk 757 of 943\n",
      "Requesting chunk 758 of 943\n",
      "Requesting chunk 759 of 943\n",
      "Requesting chunk 760 of 943\n",
      "Requesting chunk 761 of 943\n",
      "Requesting chunk 762 of 943\n",
      "Requesting chunk 763 of 943\n",
      "Requesting chunk 764 of 943\n",
      "Requesting chunk 765 of 943\n",
      "Requesting chunk 766 of 943\n",
      "Requesting chunk 767 of 943\n",
      "Requesting chunk 768 of 943\n",
      "Requesting chunk 769 of 943\n",
      "Requesting chunk 770 of 943\n",
      "Requesting chunk 771 of 943\n",
      "Requesting chunk 772 of 943\n",
      "Requesting chunk 773 of 943\n",
      "Requesting chunk 774 of 943\n",
      "Requesting chunk 775 of 943\n",
      "Requesting chunk 776 of 943\n",
      "Requesting chunk 777 of 943\n",
      "Requesting chunk 778 of 943\n",
      "Requesting chunk 779 of 943\n",
      "Requesting chunk 780 of 943\n",
      "Requesting chunk 781 of 943\n",
      "Requesting chunk 782 of 943\n",
      "Requesting chunk 783 of 943\n",
      "Requesting chunk 784 of 943\n",
      "Requesting chunk 785 of 943\n",
      "Requesting chunk 786 of 943\n",
      "Requesting chunk 787 of 943\n",
      "Requesting chunk 788 of 943\n",
      "Requesting chunk 789 of 943\n",
      "Requesting chunk 790 of 943\n",
      "Requesting chunk 791 of 943\n",
      "Requesting chunk 792 of 943\n",
      "Requesting chunk 793 of 943\n",
      "Requesting chunk 794 of 943\n",
      "Requesting chunk 795 of 943\n",
      "Requesting chunk 796 of 943\n",
      "Requesting chunk 797 of 943\n",
      "Requesting chunk 798 of 943\n",
      "Requesting chunk 799 of 943\n",
      "Requesting chunk 800 of 943\n",
      "Requesting chunk 801 of 943\n",
      "Requesting chunk 802 of 943\n",
      "Requesting chunk 803 of 943\n",
      "Requesting chunk 804 of 943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting chunk 805 of 943\n",
      "Requesting chunk 806 of 943\n",
      "Requesting chunk 807 of 943\n",
      "Requesting chunk 808 of 943\n",
      "Requesting chunk 809 of 943\n",
      "Requesting chunk 810 of 943\n",
      "Requesting chunk 811 of 943\n",
      "Requesting chunk 812 of 943\n",
      "Requesting chunk 813 of 943\n",
      "Requesting chunk 814 of 943\n",
      "Requesting chunk 815 of 943\n",
      "Requesting chunk 816 of 943\n",
      "Requesting chunk 817 of 943\n",
      "Requesting chunk 818 of 943\n",
      "Requesting chunk 819 of 943\n",
      "Requesting chunk 820 of 943\n",
      "Requesting chunk 821 of 943\n",
      "Requesting chunk 822 of 943\n",
      "Requesting chunk 823 of 943\n",
      "Requesting chunk 824 of 943\n",
      "Requesting chunk 825 of 943\n",
      "Requesting chunk 826 of 943\n",
      "Requesting chunk 827 of 943\n",
      "Requesting chunk 828 of 943\n",
      "Requesting chunk 829 of 943\n",
      "Requesting chunk 830 of 943\n",
      "Requesting chunk 831 of 943\n",
      "Requesting chunk 832 of 943\n",
      "Requesting chunk 833 of 943\n",
      "Requesting chunk 834 of 943\n",
      "Requesting chunk 835 of 943\n",
      "Requesting chunk 836 of 943\n",
      "Requesting chunk 837 of 943\n",
      "Requesting chunk 838 of 943\n",
      "Requesting chunk 839 of 943\n",
      "Requesting chunk 840 of 943\n",
      "Requesting chunk 841 of 943\n",
      "Requesting chunk 842 of 943\n",
      "Requesting chunk 843 of 943\n",
      "Requesting chunk 844 of 943\n",
      "Requesting chunk 845 of 943\n",
      "Requesting chunk 846 of 943\n",
      "Requesting chunk 847 of 943\n",
      "Requesting chunk 848 of 943\n",
      "Requesting chunk 849 of 943\n",
      "Requesting chunk 850 of 943\n",
      "Requesting chunk 851 of 943\n",
      "Requesting chunk 852 of 943\n",
      "Requesting chunk 853 of 943\n",
      "Requesting chunk 854 of 943\n",
      "Requesting chunk 855 of 943\n",
      "Requesting chunk 856 of 943\n",
      "Requesting chunk 857 of 943\n",
      "Requesting chunk 858 of 943\n",
      "Requesting chunk 859 of 943\n",
      "Requesting chunk 860 of 943\n",
      "Requesting chunk 861 of 943\n",
      "Requesting chunk 862 of 943\n",
      "Requesting chunk 863 of 943\n",
      "Requesting chunk 864 of 943\n",
      "Requesting chunk 865 of 943\n",
      "Requesting chunk 866 of 943\n",
      "Requesting chunk 867 of 943\n",
      "Requesting chunk 868 of 943\n",
      "Requesting chunk 869 of 943\n",
      "Requesting chunk 870 of 943\n",
      "Requesting chunk 871 of 943\n",
      "Requesting chunk 872 of 943\n",
      "Requesting chunk 873 of 943\n",
      "Requesting chunk 874 of 943\n",
      "Requesting chunk 875 of 943\n",
      "Requesting chunk 876 of 943\n",
      "Requesting chunk 877 of 943\n",
      "Requesting chunk 878 of 943\n",
      "Requesting chunk 879 of 943\n",
      "Requesting chunk 880 of 943\n",
      "Requesting chunk 881 of 943\n",
      "Requesting chunk 882 of 943\n",
      "Requesting chunk 883 of 943\n",
      "Requesting chunk 884 of 943\n",
      "Requesting chunk 885 of 943\n",
      "Requesting chunk 886 of 943\n",
      "Requesting chunk 887 of 943\n",
      "Requesting chunk 888 of 943\n",
      "Requesting chunk 889 of 943\n",
      "Requesting chunk 890 of 943\n",
      "Requesting chunk 891 of 943\n",
      "Requesting chunk 892 of 943\n",
      "Requesting chunk 893 of 943\n",
      "Requesting chunk 894 of 943\n",
      "Requesting chunk 895 of 943\n",
      "Requesting chunk 896 of 943\n",
      "Requesting chunk 897 of 943\n",
      "Requesting chunk 898 of 943\n",
      "Requesting chunk 899 of 943\n",
      "Requesting chunk 900 of 943\n",
      "Requesting chunk 901 of 943\n",
      "Requesting chunk 902 of 943\n",
      "Requesting chunk 903 of 943\n",
      "Requesting chunk 904 of 943\n",
      "Requesting chunk 905 of 943\n",
      "Requesting chunk 906 of 943\n",
      "Requesting chunk 907 of 943\n",
      "Requesting chunk 908 of 943\n",
      "Requesting chunk 909 of 943\n",
      "Requesting chunk 910 of 943\n",
      "Requesting chunk 911 of 943\n",
      "Requesting chunk 912 of 943\n",
      "Requesting chunk 913 of 943\n",
      "Requesting chunk 914 of 943\n",
      "Requesting chunk 915 of 943\n",
      "Requesting chunk 916 of 943\n",
      "Requesting chunk 917 of 943\n",
      "Requesting chunk 918 of 943\n",
      "Requesting chunk 919 of 943\n",
      "Requesting chunk 920 of 943\n",
      "Requesting chunk 921 of 943\n",
      "Requesting chunk 922 of 943\n",
      "Requesting chunk 923 of 943\n",
      "Requesting chunk 924 of 943\n",
      "Requesting chunk 925 of 943\n",
      "Requesting chunk 926 of 943\n",
      "Requesting chunk 927 of 943\n",
      "Requesting chunk 928 of 943\n",
      "Requesting chunk 929 of 943\n",
      "Requesting chunk 930 of 943\n",
      "Requesting chunk 931 of 943\n",
      "Requesting chunk 932 of 943\n",
      "Requesting chunk 933 of 943\n",
      "Requesting chunk 934 of 943\n",
      "Requesting chunk 935 of 943\n",
      "Requesting chunk 936 of 943\n",
      "Requesting chunk 937 of 943\n",
      "Requesting chunk 938 of 943\n",
      "Requesting chunk 939 of 943\n",
      "Requesting chunk 940 of 943\n",
      "Requesting chunk 941 of 943\n",
      "Requesting chunk 942 of 943\n",
      "Requesting chunk 943 of 943\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from ratelimit import limits\n",
    "\n",
    "final_ores_df = pd.DataFrame()\n",
    "\n",
    "# Customize these with your own information\n",
    "headers = {\n",
    "    'User-Agent': 'https://github.com/Arne117',\n",
    "    'From': 'arner92@zedat.fu-berlin.de'\n",
    "}\n",
    "\n",
    "# 50 revisions within a given request, up to 4 parallel requests.\n",
    "@limits(calls=4, period=0.1)\n",
    "def get_ores_data(rev_ids, headers):\n",
    "    \n",
    "    # Define the endpoint\n",
    "    # https://ores.wikimedia.org/scores/enwiki/?models=wp10&revids=807420979|807422778\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "\n",
    "    params = {\n",
    "        'project' : 'enwiki',\n",
    "        'model'   : 'wp10',\n",
    "        'revids'  : rev_ids\n",
    "    }\n",
    "\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    response = api_call.json()\n",
    "    data = json.loads(json.dumps(response))\n",
    "\n",
    "    return data\n",
    "\n",
    "def clean_ores_data(data):\n",
    "    del data['enwiki']['models']\n",
    "    chunk_df = pd.DataFrame(data['enwiki'])\n",
    "    chunk_df.columns = ['score']\n",
    "    chunk_df.index.name = 'rev_id'\n",
    "    chunk_df['score'] = chunk_df['score'].apply(lambda score: score['wp10'])\n",
    "    return chunk_df\n",
    "\n",
    "#chunk1 = '355319463|393276188|393822005|395521877|395526568|401577829|442937236|448555418|470173494|477962574|492060822|492964343|498683267|502721672|516633096|521986779|532253442|543225630|545936100|546364151|549300521|550682925|550953646|559553872|559788982|560758943|561744402|564873005|565745353|565745365|565745375|566504165|573710096|574571582|576988466|585894477|592289232|595693452|596181202|598819900|601122766|601127343|614786300|623004627|623334577|624468970|625509885|626606789|627001041|627051151'\n",
    "#chunk2 = '627432937|627547024|628261896|628268705|628270736|628312759|628379479|628563978|628619000|628766656|628988952|629562076|629818376|630396351|630396786|630704768|631437331|631581752|632008524|632261377|632447328|633612729|634032715|635240253|635814126|636911471|637801253|638214719|638362866|638377138|638566016|638571205|638599355|639021339|639061161|639471171|640014648|640214913|640826254|641422326|643410335|643746000|643932216|643932220|643932225|643932226|643932239|643932242|644024203|644041882'\n",
    "#chunk3 = '644399375|645225697|645408814|647367482|647450883|647483021|647832959|647893858|648048611|648271473|650458255|650462344|650494773|651250302|651785828|651856758|653467222|653527941|653895210|654012510|655031906|655032578|655390291|655980284|656352211|656386131|656737492|657176628|658939122|659315526|660214915|660579437|660884852|662681733|662927043|663088604|663166199|663348266|663572469|663617582|663783398|663880497|664532059|664669156|664787336|664793790|664814622|664822251|666029819|666596918'\n",
    "\n",
    "#chunks = [chunk1, chunk2, chunk1]\n",
    "#for chunk in chunks:\n",
    "#    chunk_ores_result = get_ores_data(chunk, headers)\n",
    "#    chunk_ores_df = clean_ores_data(chunk_ores_result)\n",
    "#    final_ores_df = pd.concat([final_ores_df, chunk_ores_df])\n",
    "\n",
    "# The last rows index gives the length of the df and is dived by 50 to get the number of chunks needed to fit 50 rev_ids into one chunk for the api call\n",
    "for i, chunk in enumerate(np.array_split(articles_df['rev_id'], int(articles_df.iloc[-1].name / 50)), start=1):\n",
    "    print(f\"Requesting chunk {i} of {int(articles_df.iloc[-1].name / 50)}\")\n",
    "    chunk_rev_ids = '|'.join(map(str, chunk))\n",
    "    #chunk_ores_result = get_ores_data(chunk_rev_ids, headers)\n",
    "    chunk_ores_df = clean_ores_data(chunk_ores_result)\n",
    "    final_ores_df = pd.concat([final_ores_df, chunk_ores_df]) # ignore_index=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_ores_df.to_csv('en-wikipedia_tmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "ores_df = pd.read_csv('en-wikipedia_tmp.csv', index_col='rev_id')\n",
    "ores_df['score'] = ores_df['score'].str.replace(\"\\'\", \"\\\"\")\n",
    "ores_df['score'] = ores_df['score'].apply(lambda score: json.loads(score))\n",
    "# ores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending one request for each `rev_id` might take some time. If you want to send batches you can use `'|'.join(str(x) for x in revision_ids` to put your ids together. Please make sure to deal with [exception handling](https://www.w3schools.com/python/python_try_except.asp) of the `KeyError` exception, when extracting the `prediction` from the `JSON` response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets\n",
    "\n",
    "Now you need to combine both dataset: (1) the wikipedia articles and its ORES quality scores and (2) the population data. Both have columns named `country`. After merging the data, you'll invariably run into entries which cannot be merged. Either the population dataset does not have an entry for the equivalent Wikipedia country, or vis versa.\n",
    "\n",
    "Please remove any rows that do not have matching data, and output them to a `CSV` file called `countries-no_match.csv`. Consolidate the remaining data into a single `CSV` file called `politicians_by_country.csv`.\n",
    "\n",
    "The schema for that file should look like the following table:\n",
    "\n",
    "\n",
    "| article_name | country | region | revision_id | article_quality | population |\n",
    "|--------------|---------|--------|-------------|-----------------|------------|\n",
    "| Bir I of Kanem | Chad  | AFRICA | 807422778 | Stub | 16877000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we combine the articles data frame with the ORES result data frame for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Information Minister of the Palestinian Nation...</td>\n",
       "      <td>Palestinian Territory</td>\n",
       "      <td>393276188</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yos Por</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>393822005</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Julius Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395521877</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Edvard Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395526568</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47192</th>\n",
       "      <td>Yahya Jammeh</td>\n",
       "      <td>Gambia</td>\n",
       "      <td>807482007</td>\n",
       "      <td>{'score': {'prediction': 'GA', 'probability': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47193</th>\n",
       "      <td>Lucius Fairchild</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483006</td>\n",
       "      <td>{'score': {'prediction': 'C', 'probability': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47194</th>\n",
       "      <td>Fahd of Saudi Arabia</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>807483153</td>\n",
       "      <td>{'score': {'prediction': 'GA', 'probability': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47195</th>\n",
       "      <td>Francis Fessenden</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483270</td>\n",
       "      <td>{'score': {'prediction': 'C', 'probability': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47196</th>\n",
       "      <td>Ajay Kannoujiya</td>\n",
       "      <td>India</td>\n",
       "      <td>807484325</td>\n",
       "      <td>{'error': {'message': 'RevisionNotFound: Could...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46701 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    page  \\\n",
       "1                                         Bir I of Kanem   \n",
       "10     Information Minister of the Palestinian Nation...   \n",
       "12                                               Yos Por   \n",
       "23                                          Julius Gregr   \n",
       "24                                          Edvard Gregr   \n",
       "...                                                  ...   \n",
       "47192                                       Yahya Jammeh   \n",
       "47193                                   Lucius Fairchild   \n",
       "47194                               Fahd of Saudi Arabia   \n",
       "47195                                  Francis Fessenden   \n",
       "47196                                    Ajay Kannoujiya   \n",
       "\n",
       "                     country     rev_id  \\\n",
       "1                       Chad  355319463   \n",
       "10     Palestinian Territory  393276188   \n",
       "12                  Cambodia  393822005   \n",
       "23            Czech Republic  395521877   \n",
       "24            Czech Republic  395526568   \n",
       "...                      ...        ...   \n",
       "47192                 Gambia  807482007   \n",
       "47193          United States  807483006   \n",
       "47194           Saudi Arabia  807483153   \n",
       "47195          United States  807483270   \n",
       "47196                  India  807484325   \n",
       "\n",
       "                                                   score  \n",
       "1      {'score': {'prediction': 'Stub', 'probability'...  \n",
       "10     {'score': {'prediction': 'Stub', 'probability'...  \n",
       "12     {'score': {'prediction': 'Stub', 'probability'...  \n",
       "23     {'score': {'prediction': 'Stub', 'probability'...  \n",
       "24     {'score': {'prediction': 'Stub', 'probability'...  \n",
       "...                                                  ...  \n",
       "47192  {'score': {'prediction': 'GA', 'probability': ...  \n",
       "47193  {'score': {'prediction': 'C', 'probability': {...  \n",
       "47194  {'score': {'prediction': 'GA', 'probability': ...  \n",
       "47195  {'score': {'prediction': 'C', 'probability': {...  \n",
       "47196  {'error': {'message': 'RevisionNotFound: Could...  \n",
       "\n",
       "[46701 rows x 4 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = articles_df.join(ores_df, on='rev_id')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>score</th>\n",
       "      <th>population</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "      <td>16.877</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Information Minister of the Palestinian Nation...</td>\n",
       "      <td>Palestinian Territory</td>\n",
       "      <td>393276188</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "      <td>5.008</td>\n",
       "      <td>ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yos Por</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>393822005</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "      <td>15.497</td>\n",
       "      <td>ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julius Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395521877</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edvard Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395526568</td>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46696</th>\n",
       "      <td>Yahya Jammeh</td>\n",
       "      <td>Gambia</td>\n",
       "      <td>807482007</td>\n",
       "      <td>{'score': {'prediction': 'GA', 'probability': ...</td>\n",
       "      <td>2.417</td>\n",
       "      <td>AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46697</th>\n",
       "      <td>Lucius Fairchild</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483006</td>\n",
       "      <td>{'score': {'prediction': 'C', 'probability': {...</td>\n",
       "      <td>329.878</td>\n",
       "      <td>NORTHERN AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46698</th>\n",
       "      <td>Fahd of Saudi Arabia</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>807483153</td>\n",
       "      <td>{'score': {'prediction': 'GA', 'probability': ...</td>\n",
       "      <td>35.041</td>\n",
       "      <td>ASIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46699</th>\n",
       "      <td>Francis Fessenden</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483270</td>\n",
       "      <td>{'score': {'prediction': 'C', 'probability': {...</td>\n",
       "      <td>329.878</td>\n",
       "      <td>NORTHERN AMERICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46700</th>\n",
       "      <td>Ajay Kannoujiya</td>\n",
       "      <td>India</td>\n",
       "      <td>807484325</td>\n",
       "      <td>{'error': {'message': 'RevisionNotFound: Could...</td>\n",
       "      <td>1400.100</td>\n",
       "      <td>ASIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46701 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    page  \\\n",
       "0                                         Bir I of Kanem   \n",
       "1      Information Minister of the Palestinian Nation...   \n",
       "2                                                Yos Por   \n",
       "3                                           Julius Gregr   \n",
       "4                                           Edvard Gregr   \n",
       "...                                                  ...   \n",
       "46696                                       Yahya Jammeh   \n",
       "46697                                   Lucius Fairchild   \n",
       "46698                               Fahd of Saudi Arabia   \n",
       "46699                                  Francis Fessenden   \n",
       "46700                                    Ajay Kannoujiya   \n",
       "\n",
       "                     country     rev_id  \\\n",
       "0                       Chad  355319463   \n",
       "1      Palestinian Territory  393276188   \n",
       "2                   Cambodia  393822005   \n",
       "3             Czech Republic  395521877   \n",
       "4             Czech Republic  395526568   \n",
       "...                      ...        ...   \n",
       "46696                 Gambia  807482007   \n",
       "46697          United States  807483006   \n",
       "46698           Saudi Arabia  807483153   \n",
       "46699          United States  807483270   \n",
       "46700                  India  807484325   \n",
       "\n",
       "                                                   score  population  \\\n",
       "0      {'score': {'prediction': 'Stub', 'probability'...      16.877   \n",
       "1      {'score': {'prediction': 'Stub', 'probability'...       5.008   \n",
       "2      {'score': {'prediction': 'Stub', 'probability'...      15.497   \n",
       "3      {'score': {'prediction': 'Stub', 'probability'...         NaN   \n",
       "4      {'score': {'prediction': 'Stub', 'probability'...         NaN   \n",
       "...                                                  ...         ...   \n",
       "46696  {'score': {'prediction': 'GA', 'probability': ...       2.417   \n",
       "46697  {'score': {'prediction': 'C', 'probability': {...     329.878   \n",
       "46698  {'score': {'prediction': 'GA', 'probability': ...      35.041   \n",
       "46699  {'score': {'prediction': 'C', 'probability': {...     329.878   \n",
       "46700  {'error': {'message': 'RevisionNotFound: Could...    1400.100   \n",
       "\n",
       "                 region  \n",
       "0                AFRICA  \n",
       "1                  ASIA  \n",
       "2                  ASIA  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "...                 ...  \n",
       "46696            AFRICA  \n",
       "46697  NORTHERN AMERICA  \n",
       "46698              ASIA  \n",
       "46699  NORTHERN AMERICA  \n",
       "46700              ASIA  \n",
       "\n",
       "[46701 rows x 6 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = merged_df.merge(population_df, on='country', how='left')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's possible that you will be unable to get a score for a particular article.\n",
    "# If that happens, make sure to maintain a log of articles for which you were not able to retrieve an ORES score.\n",
    "# This log should be saved as a separate file named ORES_no_scores.csv and should include the page, country, and\n",
    "# rev_id (just as in page_data.csv).\n",
    "ores_no_score_mask = merged_df['score'].apply(lambda res: 'error' in res)\n",
    "ores_score_mask = merged_df['score'].apply(lambda res: 'score' in res)\n",
    "\n",
    "ores_error_df = merged_df[ores_no_score_mask]\n",
    "ores_score_df = merged_df[ores_score_mask]\n",
    "#ores_error_df\n",
    "#ores_score_df\n",
    "\n",
    "ores_error_df[['page', 'country', 'rev_id', 'score']].to_csv('../data_clean/ORES_no_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-236-38485ddf7827>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['article_quality'] = final_df['score'].apply(lambda entry: entry['score']['prediction'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_name</th>\n",
       "      <th>country</th>\n",
       "      <th>region</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>article_quality</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>355319463</td>\n",
       "      <td>Stub</td>\n",
       "      <td>16.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Information Minister of the Palestinian Nation...</td>\n",
       "      <td>Palestinian Territory</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>393276188</td>\n",
       "      <td>Stub</td>\n",
       "      <td>5.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yos Por</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>393822005</td>\n",
       "      <td>Stub</td>\n",
       "      <td>15.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Julius Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>395521877</td>\n",
       "      <td>Stub</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edvard Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>395526568</td>\n",
       "      <td>Stub</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46695</th>\n",
       "      <td>Hal Bidlack</td>\n",
       "      <td>United States</td>\n",
       "      <td>NORTHERN AMERICA</td>\n",
       "      <td>807481636</td>\n",
       "      <td>C</td>\n",
       "      <td>329.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46696</th>\n",
       "      <td>Yahya Jammeh</td>\n",
       "      <td>Gambia</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>807482007</td>\n",
       "      <td>GA</td>\n",
       "      <td>2.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46697</th>\n",
       "      <td>Lucius Fairchild</td>\n",
       "      <td>United States</td>\n",
       "      <td>NORTHERN AMERICA</td>\n",
       "      <td>807483006</td>\n",
       "      <td>C</td>\n",
       "      <td>329.878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46698</th>\n",
       "      <td>Fahd of Saudi Arabia</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>ASIA</td>\n",
       "      <td>807483153</td>\n",
       "      <td>GA</td>\n",
       "      <td>35.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46699</th>\n",
       "      <td>Francis Fessenden</td>\n",
       "      <td>United States</td>\n",
       "      <td>NORTHERN AMERICA</td>\n",
       "      <td>807483270</td>\n",
       "      <td>C</td>\n",
       "      <td>329.878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46478 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_name  \\\n",
       "0                                         Bir I of Kanem   \n",
       "1      Information Minister of the Palestinian Nation...   \n",
       "2                                                Yos Por   \n",
       "3                                           Julius Gregr   \n",
       "4                                           Edvard Gregr   \n",
       "...                                                  ...   \n",
       "46695                                        Hal Bidlack   \n",
       "46696                                       Yahya Jammeh   \n",
       "46697                                   Lucius Fairchild   \n",
       "46698                               Fahd of Saudi Arabia   \n",
       "46699                                  Francis Fessenden   \n",
       "\n",
       "                     country            region     rev_id article_quality  \\\n",
       "0                       Chad            AFRICA  355319463            Stub   \n",
       "1      Palestinian Territory              ASIA  393276188            Stub   \n",
       "2                   Cambodia              ASIA  393822005            Stub   \n",
       "3             Czech Republic               NaN  395521877            Stub   \n",
       "4             Czech Republic               NaN  395526568            Stub   \n",
       "...                      ...               ...        ...             ...   \n",
       "46695          United States  NORTHERN AMERICA  807481636               C   \n",
       "46696                 Gambia            AFRICA  807482007              GA   \n",
       "46697          United States  NORTHERN AMERICA  807483006               C   \n",
       "46698           Saudi Arabia              ASIA  807483153              GA   \n",
       "46699          United States  NORTHERN AMERICA  807483270               C   \n",
       "\n",
       "       population  \n",
       "0          16.877  \n",
       "1           5.008  \n",
       "2          15.497  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "46695     329.878  \n",
       "46696       2.417  \n",
       "46697     329.878  \n",
       "46698      35.041  \n",
       "46699     329.878  \n",
       "\n",
       "[46478 rows x 6 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unused dictonary structure and format the dataframe to the required structure\n",
    "final_df = ores_score_df\n",
    "final_df['article_quality'] = final_df['score'].apply(lambda entry: entry['score']['prediction'])\n",
    "final_df = final_df.rename(columns={'page': 'article_name'})\n",
    "final_df = final_df.drop('score', 1)\n",
    "final_df = final_df[['article_name', 'country', 'region', 'rev_id', 'article_quality', 'population']]\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3⃣ | Analysis\n",
    "\n",
    "Your analysis will consist of calculating the proportion (as a percentage) of articles-per-population (we can also call it `coverage`) and high-quality articles (we can also call it `relative-quality`)for **each country** and for **each region**. By `\"high quality\"` arcticle we mean an article that ORES predicted as `FA` (featured article) or `GA` (good article).\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* if a country has a population of `10,000` people, and you found `10` articles about politicians from that country, then the percentage of `articles-per-population` would be `0.1%`.\n",
    "* if a country has `10` articles about politicians, and `2` of them are `FA` or `GA` class articles, then the percentage of `high-quality-articles` would be `20%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results format\n",
    "\n",
    "The results from this analysis are six `data tables`. Embed these tables in the Jupyter notebook. You do not need to graph or otherwise visualize the data for this assignment. The tables will show:\n",
    "\n",
    "1. **Top 10 countries by coverage**<br>10 highest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "1. **Bottom 10 countries by coverage**<br>10 lowest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "1. **Top 10 countries by relative quality**<br>10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "1. **Bottom 10 countries by relative quality**<br>10 lowest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "1. **Regions by coverage**<br>Ranking of regions (in descending order) in terms of the total count of politician articles from countries in each region as a proportion of total regional population\n",
    "1. **Regions by coverage**<br>Ranking of regions (in descending order) in terms of the relative proportion of politician articles from countries in each region that are of GA and FA-quality\n",
    "\n",
    "**❗Hint:** You will find what country belongs to which region (e.g. `ASIA`) also in `export_2019.csv`. You need to calculate the total poulation per region. For that you could use `groupby` and also check out `apply`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Top 10 countries by coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x11da49f10>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_coverage = final_df.groupby('country')\n",
    "\n",
    "# sorting dataframe \n",
    "data.sort_values('team', inplace = True) \n",
    "  \n",
    "# making boolean series for a team name \n",
    "filter1 = data[\"Team\"]==\"Atlanta Hawks\"\n",
    "  \n",
    "# making boolean series for age \n",
    "filter2 = final_df['ga'] +  final_df['fa'] >= 0.5\n",
    "  \n",
    "# filtering data on basis of both filters \n",
    "data.where(filter1 & filter2, inplace = True) \n",
    "countries_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ga + fa >= 0.5\n",
    "filter2 = final_df['ga'] +  final_df['fa'] >= 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Credits\n",
    "\n",
    "This exercise is slighty adapted from the course [Human Centered Data Science (Fall 2019)](https://wiki.communitydata.science/Human_Centered_Data_Science_(Fall_2019)) of [Univeristy of Washington](https://www.washington.edu/datasciencemasters/) by [Jonathan T. Morgan](https://wiki.communitydata.science/User:Jtmorgan).\n",
    "\n",
    "Same as the original inventors, we release the notebooks under the [Creative Commons Attribution license (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
