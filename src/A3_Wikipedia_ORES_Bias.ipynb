{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Course Human-Centered Data Science ([HCDS](https://www.mi.fu-berlin.de/en/inf/groups/hcc/teaching/winter_term_2020_21/course_human_centered_data_science.html)) - Winter Term 2020/21 - [HCC](https://www.mi.fu-berlin.de/en/inf/groups/hcc/index.html) | [Freie Universität Berlin](https://www.fu-berlin.de/)\n",
    "\n",
    "***\n",
    "\n",
    "# A2 - Wikipedia, ORES, and Bias in Data\n",
    "Please follow the reproducability workflow as practiced during the last exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1⃣ | Data acquisition\n",
    "\n",
    "You will use two data sources: (1) Wikipedia articles of politicians and (2) world population data.\n",
    "\n",
    "**Wikipedia articles -**\n",
    "The Wikipedia articles can be found on [Figshare](https://figshare.com/articles/Untitled_Item/5513449). It contains politiciaans by country from the English-language wikipedia. Please read through the documentation for this repository, then download and unzip it to extract the data file, which is called `page_data.csv`.\n",
    "\n",
    "**Population data -**\n",
    "The population data is available in `CSV` format in the `_data` folder. The file is named `export_2019.csv`. This dataset is drawn from the [world population datasheet](https://www.prb.org/international/indicator/population/table/) published by the Population Reference Bureau (downloaded 2020-11-13 10:14 AM). I have edited the dataset to make it easier to use in this assignment. The population per country is given in millions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step we need to import the neccessary libaries for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the csv files as pandas dataframes for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "population_df = pd.read_csv('../data_raw/export_2019.csv', delimiter=';')\n",
    "articles_df = pd.read_csv('../data_raw/page_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all files from a folder\n",
    "# files = [file for file in os.listdir('../data_raw')]\n",
    "# combined = pd.DataFrame()\n",
    "\n",
    "# for file in files:\n",
    "#     delimiter = ',' if file != 'export_2019.csv' else ';'\n",
    "#     current_df = pd.read_csv('../data_raw/' + file, delimiter=delimiter)\n",
    "#     combined = pd.concat([combined, current_df])\n",
    "# combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2⃣ | Data processing and cleaning\n",
    "The data in `page_data.csv` contain some rows that you will need to filter out. It contains some page names that start with the string `\"Template:\"`. These pages are not Wikipedia articles, and should not be included in your analysis. The data in `export_2019.csv` does not need any cleaning.\n",
    "\n",
    "***\n",
    "\n",
    "| | `page_data.csv` | | |\n",
    "|-|------|---------|--------|\n",
    "| | **page** | **country** | **rev_id** |\n",
    "|0|\tTemplate:ZambiaProvincialMinisters | Zambia | 235107991 |\n",
    "|1|\tBir I of Kanem | Chad | 355319463 |\n",
    "\n",
    "***\n",
    "\n",
    "| | `export_2019.csv` | | |\n",
    "|-|------|---------|--------|\n",
    "| | **country** | **population** | **region** |\n",
    "|0|\tAlgeria | 44.357 | AFRICA |\n",
    "|1|\tEgypt | 100.803 | 355319463 |\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean the Wikipedia articles of every row containing the `Template:` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>country</th>\n",
       "      <th>rev_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bir I of Kanem</td>\n",
       "      <td>Chad</td>\n",
       "      <td>355319463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Information Minister of the Palestinian Nation...</td>\n",
       "      <td>Palestinian Territory</td>\n",
       "      <td>393276188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yos Por</td>\n",
       "      <td>Cambodia</td>\n",
       "      <td>393822005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Julius Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395521877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Edvard Gregr</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>395526568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47192</th>\n",
       "      <td>Yahya Jammeh</td>\n",
       "      <td>Gambia</td>\n",
       "      <td>807482007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47193</th>\n",
       "      <td>Lucius Fairchild</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47194</th>\n",
       "      <td>Fahd of Saudi Arabia</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>807483153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47195</th>\n",
       "      <td>Francis Fessenden</td>\n",
       "      <td>United States</td>\n",
       "      <td>807483270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47196</th>\n",
       "      <td>Ajay Kannoujiya</td>\n",
       "      <td>India</td>\n",
       "      <td>807484325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46701 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    page  \\\n",
       "1                                         Bir I of Kanem   \n",
       "10     Information Minister of the Palestinian Nation...   \n",
       "12                                               Yos Por   \n",
       "23                                          Julius Gregr   \n",
       "24                                          Edvard Gregr   \n",
       "...                                                  ...   \n",
       "47192                                       Yahya Jammeh   \n",
       "47193                                   Lucius Fairchild   \n",
       "47194                               Fahd of Saudi Arabia   \n",
       "47195                                  Francis Fessenden   \n",
       "47196                                    Ajay Kannoujiya   \n",
       "\n",
       "                     country     rev_id  \n",
       "1                       Chad  355319463  \n",
       "10     Palestinian Territory  393276188  \n",
       "12                  Cambodia  393822005  \n",
       "23            Czech Republic  395521877  \n",
       "24            Czech Republic  395526568  \n",
       "...                      ...        ...  \n",
       "47192                 Gambia  807482007  \n",
       "47193          United States  807483006  \n",
       "47194           Saudi Arabia  807483153  \n",
       "47195          United States  807483270  \n",
       "47196                  India  807484325  \n",
       "\n",
       "[46701 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df = articles_df[~articles_df['page'].str.contains('Template:')]\n",
    "articles_df\n",
    "\n",
    "# sort out nan's\n",
    "# new_df = data_df[data_df.isna().any(axis=1)]\n",
    "# df.dropna(how='all')\n",
    "\n",
    "# convert to int\n",
    "# df['col'] = pd.to_numeric(df['col'])\n",
    "\n",
    "# run function on df\n",
    "# def get_city(address):\n",
    "#   return address.split(',')[1]\n",
    "# df['col'].apply(lambda addr: get_city(addr))\n",
    "\n",
    "# python f strings\n",
    "# .apply(lambda addr: f\"{get_city(addr)} xy\")\n",
    "\n",
    "# plot labeling\n",
    "# plt.xticks(df['x'].unique(), rotation='vertical', size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting article quality predictions with ORES\n",
    "\n",
    "Now you need to get the predicted quality scores for each article in the Wikipedia dataset. We're using a machine learning system called [**ORES**](https://www.mediawiki.org/wiki/ORES) (\"Objective Revision Evaluation Service\"). ORES estimates the quality of an article (at a particular point in time), and assigns a series of probabilities that the article is in one of the six quality categories. The options are, from best to worst:\n",
    "\n",
    "| ID | Quality Category |  Explanation |\n",
    "|----|------------------|----------|\n",
    "| 1 | FA    | Featured article |\n",
    "| 2 | GA    | Good article |\n",
    "| 3 | B     | B-class article |\n",
    "| 4 | C     | C-class article |\n",
    "| 5 | Start | Start-class article |\n",
    "| 6 | Stub  | Stub-class article |\n",
    "\n",
    "For context, these quality classes are a sub-set of quality assessment categories developed by Wikipedia editors. If you're curious, you can [read more](https://en.wikipedia.org/wiki/Wikipedia:Content_assessment#Grades) about what these assessment classes mean on English Wikipedia. For this assignment, you only need to know that these categories exist, and that ORES will assign one of these six categories to any `rev_id`. You need to extract all `rev_id`s in the `page_data.csv` file and use the ORES API to get the predicted quality score for that specific article revision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORES REST API endpoint\n",
    "\n",
    "The [ORES REST API](https://ores.wikimedia.org/v3/#!/scoring/get_v3_scores_context_revid_model) is configured fairly similarly to the pageviews API we used for the last assignment. It expects the following parameters:\n",
    "\n",
    "* **project** --> `enwiki`\n",
    "* **revid** --> e.g. `235107991` or multiple ids e.g.: `235107991|355319463` (batch)\n",
    "* **model** --> `wp10` - The name of a model to use when scoring.\n",
    "\n",
    "**❗Note on batch processing:** Please read the documentation about [API usage](https://www.mediawiki.org/wiki/ORES#API_usage) if you want to query a large number of revisions (batches). \n",
    "\n",
    "You will notice that ORES returns a prediction value that contains the name of one category (e.g. `Start`), as well as probability values for each of the six quality categories. For this assignment, you only need to capture and use the value for prediction.\n",
    "\n",
    "**❗Note:** It's possible that you will be unable to get a score for a particular article. If that happens, make sure to maintain a log of articles for which you were not able to retrieve an ORES score. This log should be saved as a separate file named `ORES_no_scores.csv` and should include the `page`, `country`, and `rev_id` (just as in `page_data.csv`).\n",
    "\n",
    "You can use the following **samle code for API calls**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355319463</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393276188</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393822005</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395521877</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395526568</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401577829</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442937236</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448555418</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470173494</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477962574</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492060822</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492964343</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498683267</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502721672</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'C', 'probab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516633096</th>\n",
       "      <td>{'wp10': {'error': {'message': 'RevisionNotFou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521986779</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532253442</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543225630</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545936100</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546364151</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549300521</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550682925</th>\n",
       "      <td>{'wp10': {'error': {'message': 'RevisionNotFou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550953646</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559553872</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559788982</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560758943</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561744402</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564873005</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565745353</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565745365</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565745375</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566504165</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573710096</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'B', 'probab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574571582</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576988466</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585894477</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592289232</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595693452</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596181202</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598819900</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601122766</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601127343</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614786300</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623004627</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623334577</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624468970</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625509885</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Start', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626606789</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627001041</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627051151</th>\n",
       "      <td>{'wp10': {'score': {'prediction': 'Stub', 'pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      scores\n",
       "355319463  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "393276188  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "393822005  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "395521877  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "395526568  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "401577829  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "442937236  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "448555418  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "470173494  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "477962574  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "492060822  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "492964343  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "498683267  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "502721672  {'wp10': {'score': {'prediction': 'C', 'probab...\n",
       "516633096  {'wp10': {'error': {'message': 'RevisionNotFou...\n",
       "521986779  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "532253442  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "543225630  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "545936100  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "546364151  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "549300521  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "550682925  {'wp10': {'error': {'message': 'RevisionNotFou...\n",
       "550953646  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "559553872  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "559788982  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "560758943  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "561744402  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "564873005  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "565745353  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "565745365  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "565745375  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "566504165  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "573710096  {'wp10': {'score': {'prediction': 'B', 'probab...\n",
       "574571582  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "576988466  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "585894477  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "592289232  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "595693452  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "596181202  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "598819900  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "601122766  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "601127343  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "614786300  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "623004627  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "623334577  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "624468970  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "625509885  {'wp10': {'score': {'prediction': 'Start', 'pr...\n",
       "626606789  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "627001041  {'wp10': {'score': {'prediction': 'Stub', 'pro...\n",
       "627051151  {'wp10': {'score': {'prediction': 'Stub', 'pro..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from ratelimit import limits\n",
    "\n",
    "# Customize these with your own information\n",
    "headers = {\n",
    "    'User-Agent': 'https://github.com/Arne117',\n",
    "    'From': 'arner92@zedat.fu-berlin.de'\n",
    "}\n",
    "\n",
    "# 50 revisions within a given request, up to 4 parallel requests.\n",
    "@limits(calls=4, period=0.1)\n",
    "def get_ores_data(rev_ids, headers):\n",
    "    \n",
    "    # Define the endpoint\n",
    "    # https://ores.wikimedia.org/scores/enwiki/?models=wp10&revids=807420979|807422778\n",
    "    endpoint = 'https://ores.wikimedia.org/v3/scores/{project}/?models={model}&revids={revids}'\n",
    "\n",
    "    params = {\n",
    "        'project' : 'enwiki',\n",
    "        'model'   : 'wp10',\n",
    "        'revids'  : rev_ids\n",
    "    }\n",
    "\n",
    "    api_call = requests.get(endpoint.format(**params))\n",
    "    response = api_call.json()\n",
    "    data = json.loads(json.dumps(response))\n",
    "\n",
    "    return data\n",
    "\n",
    "result = get_ores_data('355319463|393276188|393822005|395521877|395526568|401577829|442937236|448555418|470173494|477962574|492060822|492964343|498683267|502721672|516633096|521986779|532253442|543225630|545936100|546364151|549300521|550682925|550953646|559553872|559788982|560758943|561744402|564873005|565745353|565745365|565745375|566504165|573710096|574571582|576988466|585894477|592289232|595693452|596181202|598819900|601122766|601127343|614786300|623004627|623334577|624468970|625509885|626606789|627001041|627051151', headers)\n",
    "del result['enwiki']['models']\n",
    "\n",
    "test_df = pd.DataFrame(result['enwiki'])\n",
    "test_df.columns = ['score']\n",
    "test_df.index.name = 'rev_id'\n",
    "test_df['score'] = test_df['score'].apply(lambda score: score['wp10'])\n",
    "test_df\n",
    "\n",
    "# The last rows index gives the length of the df and is dived by 50 to get the number of chunks needed to fit 50 rev_ids into one chunk for the api call\n",
    "#for chunk in np.array_split(articles_df['rev_id'], int(articles_df.iloc[-1].name / 50)):\n",
    "#    print('|'.join(map(str, chunk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355319463</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393276188</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393822005</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395521877</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395526568</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401577829</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442937236</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448555418</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470173494</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477962574</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492060822</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492964343</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498683267</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502721672</th>\n",
       "      <td>{'score': {'prediction': 'C', 'probability': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516633096</th>\n",
       "      <td>{'error': {'message': 'RevisionNotFound: Could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521986779</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532253442</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543225630</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545936100</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546364151</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549300521</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550682925</th>\n",
       "      <td>{'error': {'message': 'RevisionNotFound: Could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550953646</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559553872</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559788982</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560758943</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561744402</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564873005</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565745353</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565745365</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565745375</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566504165</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573710096</th>\n",
       "      <td>{'score': {'prediction': 'B', 'probability': {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574571582</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576988466</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585894477</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592289232</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595693452</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596181202</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598819900</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601122766</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601127343</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614786300</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623004627</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623334577</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624468970</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625509885</th>\n",
       "      <td>{'score': {'prediction': 'Start', 'probability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626606789</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627001041</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627051151</th>\n",
       "      <td>{'score': {'prediction': 'Stub', 'probability'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       score\n",
       "rev_id                                                      \n",
       "355319463  {'score': {'prediction': 'Stub', 'probability'...\n",
       "393276188  {'score': {'prediction': 'Stub', 'probability'...\n",
       "393822005  {'score': {'prediction': 'Stub', 'probability'...\n",
       "395521877  {'score': {'prediction': 'Stub', 'probability'...\n",
       "395526568  {'score': {'prediction': 'Stub', 'probability'...\n",
       "401577829  {'score': {'prediction': 'Stub', 'probability'...\n",
       "442937236  {'score': {'prediction': 'Stub', 'probability'...\n",
       "448555418  {'score': {'prediction': 'Stub', 'probability'...\n",
       "470173494  {'score': {'prediction': 'Start', 'probability...\n",
       "477962574  {'score': {'prediction': 'Stub', 'probability'...\n",
       "492060822  {'score': {'prediction': 'Stub', 'probability'...\n",
       "492964343  {'score': {'prediction': 'Stub', 'probability'...\n",
       "498683267  {'score': {'prediction': 'Stub', 'probability'...\n",
       "502721672  {'score': {'prediction': 'C', 'probability': {...\n",
       "516633096  {'error': {'message': 'RevisionNotFound: Could...\n",
       "521986779  {'score': {'prediction': 'Stub', 'probability'...\n",
       "532253442  {'score': {'prediction': 'Stub', 'probability'...\n",
       "543225630  {'score': {'prediction': 'Stub', 'probability'...\n",
       "545936100  {'score': {'prediction': 'Stub', 'probability'...\n",
       "546364151  {'score': {'prediction': 'Start', 'probability...\n",
       "549300521  {'score': {'prediction': 'Stub', 'probability'...\n",
       "550682925  {'error': {'message': 'RevisionNotFound: Could...\n",
       "550953646  {'score': {'prediction': 'Stub', 'probability'...\n",
       "559553872  {'score': {'prediction': 'Stub', 'probability'...\n",
       "559788982  {'score': {'prediction': 'Stub', 'probability'...\n",
       "560758943  {'score': {'prediction': 'Stub', 'probability'...\n",
       "561744402  {'score': {'prediction': 'Stub', 'probability'...\n",
       "564873005  {'score': {'prediction': 'Stub', 'probability'...\n",
       "565745353  {'score': {'prediction': 'Stub', 'probability'...\n",
       "565745365  {'score': {'prediction': 'Stub', 'probability'...\n",
       "565745375  {'score': {'prediction': 'Stub', 'probability'...\n",
       "566504165  {'score': {'prediction': 'Stub', 'probability'...\n",
       "573710096  {'score': {'prediction': 'B', 'probability': {...\n",
       "574571582  {'score': {'prediction': 'Stub', 'probability'...\n",
       "576988466  {'score': {'prediction': 'Stub', 'probability'...\n",
       "585894477  {'score': {'prediction': 'Stub', 'probability'...\n",
       "592289232  {'score': {'prediction': 'Stub', 'probability'...\n",
       "595693452  {'score': {'prediction': 'Start', 'probability...\n",
       "596181202  {'score': {'prediction': 'Start', 'probability...\n",
       "598819900  {'score': {'prediction': 'Start', 'probability...\n",
       "601122766  {'score': {'prediction': 'Stub', 'probability'...\n",
       "601127343  {'score': {'prediction': 'Start', 'probability...\n",
       "614786300  {'score': {'prediction': 'Start', 'probability...\n",
       "623004627  {'score': {'prediction': 'Stub', 'probability'...\n",
       "623334577  {'score': {'prediction': 'Stub', 'probability'...\n",
       "624468970  {'score': {'prediction': 'Stub', 'probability'...\n",
       "625509885  {'score': {'prediction': 'Start', 'probability...\n",
       "626606789  {'score': {'prediction': 'Stub', 'probability'...\n",
       "627001041  {'score': {'prediction': 'Stub', 'probability'...\n",
       "627051151  {'score': {'prediction': 'Stub', 'probability'..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(result['enwiki'])\n",
    "test_df.columns = ['score']\n",
    "test_df.index.name = 'rev_id'\n",
    "test_df['score'] = test_df['score'].apply(lambda score: score['wp10'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending one request for each `rev_id` might take some time. If you want to send batches you can use `'|'.join(str(x) for x in revision_ids` to put your ids together. Please make sure to deal with [exception handling](https://www.w3schools.com/python/python_try_except.asp) of the `KeyError` exception, when extracting the `prediction` from the `JSON` response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the datasets\n",
    "\n",
    "Now you need to combine both dataset: (1) the wikipedia articles and its ORES quality scores and (2) the population data. Both have columns named `country`. After merging the data, you'll invariably run into entries which cannot be merged. Either the population dataset does not have an entry for the equivalent Wikipedia country, or vis versa.\n",
    "\n",
    "Please remove any rows that do not have matching data, and output them to a `CSV` file called `countries-no_match.csv`. Consolidate the remaining data into a single `CSV` file called `politicians_by_country.csv`.\n",
    "\n",
    "The schema for that file should look like the following table:\n",
    "\n",
    "\n",
    "| article_name | country | region | revision_id | article_quality | population |\n",
    "|--------------|---------|--------|-------------|-----------------|------------|\n",
    "| Bir I of Kanem | Chad  | AFRICA | 807422778 | Stub | 16877000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3⃣ | Analysis\n",
    "\n",
    "Your analysis will consist of calculating the proportion (as a percentage) of articles-per-population (we can also call it `coverage`) and high-quality articles (we can also call it `relative-quality`)for **each country** and for **each region**. By `\"high quality\"` arcticle we mean an article that ORES predicted as `FA` (featured article) or `GA` (good article).\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "* if a country has a population of `10,000` people, and you found `10` articles about politicians from that country, then the percentage of `articles-per-population` would be `0.1%`.\n",
    "* if a country has `10` articles about politicians, and `2` of them are `FA` or `GA` class articles, then the percentage of `high-quality-articles` would be `20%`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results format\n",
    "\n",
    "The results from this analysis are six `data tables`. Embed these tables in the Jupyter notebook. You do not need to graph or otherwise visualize the data for this assignment. The tables will show:\n",
    "\n",
    "1. **Top 10 countries by coverage**<br>10 highest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "1. **Bottom 10 countries by coverage**<br>10 lowest-ranked countries in terms of number of politician articles as a proportion of country population\n",
    "1. **Top 10 countries by relative quality**<br>10 highest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "1. **Bottom 10 countries by relative quality**<br>10 lowest-ranked countries in terms of the relative proportion of politician articles that are of GA and FA-quality\n",
    "1. **Regions by coverage**<br>Ranking of regions (in descending order) in terms of the total count of politician articles from countries in each region as a proportion of total regional population\n",
    "1. **Regions by coverage**<br>Ranking of regions (in descending order) in terms of the relative proportion of politician articles from countries in each region that are of GA and FA-quality\n",
    "\n",
    "**❗Hint:** You will find what country belongs to which region (e.g. `ASIA`) also in `export_2019.csv`. You need to calculate the total poulation per region. For that you could use `groupby` and also check out `apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "#### Credits\n",
    "\n",
    "This exercise is slighty adapted from the course [Human Centered Data Science (Fall 2019)](https://wiki.communitydata.science/Human_Centered_Data_Science_(Fall_2019)) of [Univeristy of Washington](https://www.washington.edu/datasciencemasters/) by [Jonathan T. Morgan](https://wiki.communitydata.science/User:Jtmorgan).\n",
    "\n",
    "Same as the original inventors, we release the notebooks under the [Creative Commons Attribution license (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
